#!/usr/bin/env python3
"""Export distilled student MLP weights to C header files for ESP32.

Generates:
  1. policy_weights.h  — FP32 weight/bias arrays for each layer
  2. golden_vectors.h  — 20 (state, expected_action) test pairs

Usage:
    python ml_training/rl/export_policy_c.py
    python ml_training/rl/export_policy_c.py --student outputs/.../student_policy.pth
    python ml_training/rl/export_policy_c.py --verify   # cross-check NumPy vs PyTorch
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path

import numpy as np
import torch
import yaml

from ml_training.rl.distill import StudentPolicy
from ml_training.rl.environment import TomatoRipeningEnv


def load_student(pth_path: Path) -> tuple[StudentPolicy, dict]:
    """Load student checkpoint and return model + metadata."""
    ckpt = torch.load(pth_path, map_location="cpu", weights_only=True)
    model = StudentPolicy(
        state_dim=ckpt["state_dim"],
        action_dim=ckpt["action_dim"],
        hidden_sizes=ckpt["hidden_sizes"],
    )
    model.load_state_dict(ckpt["model_state_dict"])
    model.eval()
    return model, ckpt


def extract_weights(model: StudentPolicy) -> list[tuple[np.ndarray, np.ndarray]]:
    """Extract (weight, bias) pairs from Sequential network."""
    layers = []
    for module in model.network:
        if hasattr(module, "weight"):
            w = module.weight.detach().numpy()
            b = module.bias.detach().numpy()
            layers.append((w, b))
    return layers


def _fmt_float_array(arr: np.ndarray, name: str) -> str:
    """Format a flat float array as a C initializer."""
    flat = arr.flatten()
    lines = [f"static const float {name}[{len(flat)}] = {{"]
    # 8 values per line
    for i in range(0, len(flat), 8):
        chunk = flat[i:i+8]
        vals = ", ".join(f"{v: .8e}" for v in chunk)
        lines.append(f"    {vals},")
    lines.append("};")
    return "\n".join(lines)


def generate_policy_weights_h(layers: list, meta: dict, out_path: Path):
    """Write policy_weights.h with weight arrays."""
    dims = []
    for i, (w, b) in enumerate(layers):
        dims.append(f"  Layer {i}: [{w.shape[1]}] -> [{w.shape[0]}]")

    header = [
        "/* Auto-generated by ml_training/rl/export_policy_c.py — DO NOT EDIT */",
        f"/* Student MLP: {meta['state_dim']}D input, {meta['action_dim']} actions */",
        f"/* Hidden sizes: {meta['hidden_sizes']} */",
        f"/* Total params: {meta['total_params']:,} */",
        f"/* Accuracy: {meta['final_accuracy']:.4f} */",
        "#pragma once",
        "",
        f"#define STUDENT_STATE_DIM  {meta['state_dim']}",
        f"#define STUDENT_ACTION_DIM {meta['action_dim']}",
        f"#define STUDENT_NUM_LAYERS {len(layers)}",
        "",
    ]

    for i, (w, b) in enumerate(layers):
        header.append(f"/* Layer {i}: [{w.shape[1]}] -> [{w.shape[0]}] */")
        header.append(f"#define LAYER{i}_IN  {w.shape[1]}")
        header.append(f"#define LAYER{i}_OUT {w.shape[0]}")
        header.append(_fmt_float_array(w, f"w{i}"))
        header.append(_fmt_float_array(b, f"b{i}"))
        header.append("")

    out_path.write_text("\n".join(header))
    print(f"✓ {out_path} ({out_path.stat().st_size / 1024:.1f} KB)")


def generate_golden_vectors(model: StudentPolicy, config: dict,
                            n_vectors: int = 20, out_path: Path = None):
    """Generate golden test vectors from actual environment rollouts."""
    vectors = []
    env = TomatoRipeningEnv(config=config, seed=12345)

    # Collect states from multiple episodes
    all_states = []
    for ep in range(5):
        obs, _ = env.reset()
        done = False
        while not done:
            all_states.append(obs.copy())
            action = model.predict(obs)
            obs, _, term, trunc, _ = env.step(int(action) if not isinstance(action, int) else action)
            done = term or trunc
        if len(all_states) >= n_vectors * 3:
            break

    # Sample evenly across the collected states
    indices = np.linspace(0, len(all_states) - 1, n_vectors, dtype=int)
    sampled = [all_states[i] for i in indices]

    # Run inference
    lines = [
        "/* Auto-generated by ml_training/rl/export_policy_c.py — DO NOT EDIT */",
        "#pragma once",
        "",
        f"#define NUM_GOLDEN_VECTORS {n_vectors}",
        f"#define GOLDEN_STATE_DIM   {len(sampled[0])}",
        "",
        f"static const float golden_states[{n_vectors}][{len(sampled[0])}] = {{",
    ]

    expected_actions = []
    for i, state in enumerate(sampled):
        with torch.no_grad():
            x = torch.FloatTensor(state).unsqueeze(0)
            logits = model(x)
            action = int(logits.argmax(dim=-1).item())
        expected_actions.append(action)
        vals = ", ".join(f"{v: .6e}" for v in state)
        lines.append(f"    {{ {vals} }},  /* -> action {action} */")

    lines.append("};")
    lines.append("")
    lines.append(f"static const uint8_t golden_actions[{n_vectors}] = {{")
    lines.append("    " + ", ".join(str(a) for a in expected_actions))
    lines.append("};")

    if out_path:
        out_path.write_text("\n".join(lines))
        print(f"✓ {out_path} ({n_vectors} vectors)")

    return sampled, expected_actions


def numpy_forward(state: np.ndarray, layers: list) -> int:
    """Pure NumPy MLP forward pass (mirrors the C implementation)."""
    x = state.copy()
    for i, (w, b) in enumerate(layers):
        x = w @ x + b
        if i < len(layers) - 1:  # ReLU for all but last layer
            x = np.maximum(x, 0.0)
    return int(np.argmax(x))


def verify(model, layers, config):
    """Cross-check PyTorch, NumPy, and C-equivalent inference."""
    states, expected = generate_golden_vectors(model, config, n_vectors=20)
    match = 0
    for i, state in enumerate(states):
        np_action = numpy_forward(np.array(state), layers)
        if np_action == expected[i]:
            match += 1
        else:
            print(f"  ✗ Vector {i}: PyTorch={expected[i]}, NumPy={np_action}")
    print(f"\n{'✓' if match == 20 else '✗'} NumPy vs PyTorch: {match}/20")
    return match == 20


def main():
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--student", type=str, default=None,
                        help="Path to student_policy.pth")
    parser.add_argument("--config", type=str, default="ml_training/config.yaml")
    parser.add_argument("--out-dir", type=str,
                        default="edge_firmware/main",
                        help="Output directory for C headers")
    parser.add_argument("--verify", action="store_true",
                        help="Cross-check NumPy vs PyTorch")
    args = parser.parse_args()

    with open(args.config) as f:
        config = yaml.safe_load(f)

    # Find latest student
    if args.student:
        pth_path = Path(args.student)
    else:
        candidates = sorted(Path("outputs").rglob("student_policy.pth"),
                            key=lambda p: p.stat().st_mtime, reverse=True)
        if not candidates:
            print("✗ No student_policy.pth found. Run distillation first.")
            return
        pth_path = candidates[0]

    print(f"Loading student from {pth_path}")
    model, meta = load_student(pth_path)
    layers = extract_weights(model)

    print(f"  Architecture: {meta['state_dim']}D → {meta['hidden_sizes']} → {meta['action_dim']}")
    print(f"  Parameters: {meta['total_params']:,}")
    print(f"  Accuracy: {meta['final_accuracy']:.4f}")

    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # Generate C headers
    generate_policy_weights_h(layers, meta, out_dir / "policy_weights.h")
    generate_golden_vectors(model, config, n_vectors=20,
                            out_path=out_dir / "golden_vectors.h")

    if args.verify:
        verify(model, layers, config)


if __name__ == "__main__":
    main()
