# Edge-RL Training Configuration
# ================================

project:
  name: "edge-rl-tomato"
  seed: 42
  output_dir: "outputs"

# --- Vision Model ---
vision:
  # Model
  model_name: "mobilenetv2"       # mobilenetv2 | mobilenetv3_small | efficientnet_lite0
  width_mult: 0.35                # MobileNetV2 width multiplier (0.35, 0.5, 1.0)
  num_classes: 4                  # Number of ripeness classes
  pretrained: true                # Use ImageNet pretrained weights
  freeze_backbone: false          # Freeze backbone during training
  input_size: 224                 # Input image size (224x224)

  # Dataset
  dataset:
    root: "data/tomato"           # Root directory for dataset
    # Kaggle dataset name (if using kagglehub)
    kaggle_dataset: "enalis/tomatoes-dataset"
    # Class names in order
    class_names:
      - "unripe"
      - "ripe"
      - "old"
      - "damaged"
    train_split: 0.70
    val_split: 0.15
    test_split: 0.15

  # Augmentation
  augmentation:
    random_horizontal_flip: 0.5
    random_vertical_flip: 0.0
    random_rotation: 15           # degrees
    color_jitter:
      brightness: 0.3
      contrast: 0.3
      saturation: 0.3
      hue: 0.1
    random_resized_crop:
      scale: [0.8, 1.0]
      ratio: [0.9, 1.1]

  # Training
  training:
    epochs: 80
    batch_size: 32
    learning_rate: 0.001
    weight_decay: 0.0001
    scheduler: "cosine"           # cosine | step | plateau
    warmup_epochs: 5
    early_stopping_patience: 15
    label_smoothing: 0.1

  # Export
  export:
    onnx_path: "outputs/models/tomato_classifier.onnx"
    espdl_path: "outputs/models/tomato_classifier.espdl"
    target_chip: "esp32s3"
    quant_bits: 8
    calibration_samples: 256

# --- RL Policy ---
rl:
  # Environment
  environment:
    temp_range: [12.5, 35.0]      # Celsius (biological safe band)
    humidity_range: [60.0, 95.0]  # Percent
    episode_days: 7               # Max episode length in days
    steps_per_day: 24             # Hourly decisions
    target_day_range: [3, 7]      # Target harvest day range
    max_pool_dim: 4               # Dimension of max-pooled spatial vector

    # State-space ablation variant
    # A (7D):  [X, dX/dt, X_ref, T, H, t_e, t_rem]  — scalar-only baseline
    # B (16D): A + [C_mu(3), C_sig(3), C_mode(3)]    — with colour statistics
    # C (20D): B + [max_pool(4)]                      — full feature set
    state_variant: "B"

    # Reward function parameters
    reward:
      lambda_rate: 0.5            # Rate-tracking penalty weight (unit: per-day)
      safety_alpha: 2.0           # Progressive safety penalty α (c = -α·min(n,cap)²)
      safety_cap: 5               # Max consecutive violations before penalty plateaus
      ripe_threshold: 0.15        # Auto-harvest X threshold (ROYG: X→0 = ripe)
      t_rem_epsilon: 0.1          # Clamping floor to avoid singularity in f(·)
      harvest_bonus: 10.0         # Terminal bonus for successful harvest
      progress_weight: 2.0        # Per-step reward for ripening progress

  # Simulator physics (ROYG convention: X=1 Green, X=0 Red)
  simulator:
    k1: 0.02                      # Ripening rate constant (day⁻¹ °C⁻¹), calibrated from Ogundiwin et al. (2022)
    t_base: 12.5                  # Base temperature (°C), per UC Davis & Saltveit (2005)
    x_min: 0.0                    # Minimum chromatic index (fully ripe)
    temp_noise_std: 0.5           # Temperature sensor noise
    humidity_noise_std: 2.0       # Humidity sensor noise
    delta_t_step: 1.0             # Incremental temp change per action (°C)
    ambient_temp_mean: 27.0       # Mean ambient temperature (°C) — Iligan City annual mean
    ambient_temp_std: 3.0         # Ambient variation — diurnal cycle (22–31°C), indoor no AC

  # DQN Training (3-action: maintain / heat / cool)
  training:
    algorithm: "DQN"
    total_timesteps: 1000000
    learning_rate: 0.0003
    buffer_size: 100000
    batch_size: 256
    gamma: 0.99
    tau: 0.005
    n_envs: 4                    # Parallel environments
    eval_freq: 10000
    eval_episodes: 20
    n_actions: 3                  # maintain, heat (+ΔT), cool (-ΔT)

  # Policy architecture
  policy:
    teacher:
      hidden_sizes: [256, 256]
    student:
      hidden_sizes: [64, 64]

  # Distillation
  distillation:
    epochs: 100
    batch_size: 512
    learning_rate: 0.001
    temperature: 3.0              # Softmax temperature
    alpha_kl: 0.7                 # KL loss weight
    alpha_mse: 0.3                # MSE loss weight
    num_samples: 100000           # Rollout samples for distillation

  # Export
  export:
    onnx_path: "outputs/models/rl_policy.onnx"
    espdl_path: "outputs/models/rl_policy.espdl"
    target_chip: "esp32s3"
    quant_bits: 8
