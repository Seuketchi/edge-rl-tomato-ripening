\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Conceptual framework of the Edge-RL system. The three-phase pipeline progresses from simulation-based reinforcement learning training through knowledge distillation and model compression to autonomous edge deployment on the ESP32-S3 microcontroller.}}{12}{figure.caption.11}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces System block diagram of the Edge-RL architecture. The ESP32-S3 microcontroller handles visual perception (Chromatic Index and RGB statistics extraction from the OV2640 camera) and autonomous control (distilled RL policy inference). Environmental feedback from the ripening chamber closes the control loop via DHT22 temperature and humidity readings.}}{25}{figure.caption.13}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Direct visual feature extraction pipeline. Raw camera frames are processed through deterministic pixel-level operations, producing a 10-dimensional continuous feature vector (3 means, 3 standard deviations, 3 modes, and the Chromatic Index) without any neural network inference.}}{27}{figure.caption.14}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces State-space ablation comparison across the three observation variants. Variant B (16D, +RGB statistics) achieves the highest mean reward and lowest timing error, while Variant A (7D) suffers a 6\% harvest failure rate without colour redundancy. Error bars represent one standard deviation across 100 evaluation episodes.}}{40}{figure.caption.22}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Algorithm comparison: action distribution and mean reward for DQN, PPO, and A2C. PPO collapsed to 79.2\% MAINTAIN, eliminating cooling entirely and accumulating safety penalties. A2C over-committed to HEAT (60.1\%), triggering repeated temperature violations. DQN alone learned a balanced diurnal strategy across all three actions, achieving the only positive mean reward.}}{42}{figure.caption.24}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Episode reward distributions across 100 evaluation episodes for all four policies. Edge-RL achieves the highest median reward with comparatively low variance. Fixed-Stage5 produces catastrophically negative rewards due to repeated safety violations. Random and Fixed-Day achieve modest positive medians but with higher variance than Edge-RL, reflecting their inability to adapt to domain-randomized conditions.}}{44}{figure.caption.26}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Representative before and after harvest photographs from real tomato trials. Left column shows the fruit at trial start (mature green stage, $X_0 \approx 0.9$). Right column shows the same fruit at system-triggered harvest ($X \leq 0.15$), confirming visible colour transition from green to red under autonomous temperature control.}}{49}{figure.caption.31}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Real-world timing error per trial compared against the simulation mean (0.67 days, dashed line). Deviations above the reference line indicate sim-to-real gap in the thermal or visual observation model; deviations below indicate that real conditions were more favourable than the domain-randomized simulator.}}{50}{figure.caption.32}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Sim-to-real action distribution comparison. Simulation values (51.7\% HEAT, 20.1\% MAINTAIN, 28.2\% COOL) are shown alongside empirical values from real tomato trials. Shifts in COOL usage are expected given differences between simulated and actual passive cooling capacity. Shifts in HEAT usage reflect differences between the Arrhenius simulator and real thermal dynamics.}}{52}{figure.caption.33}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces DQN training convergence over 1,000,000 timesteps showing episode reward (blue, smoothed) and evaluation reward (orange). The policy converges after approximately 600,000 timesteps.}}{63}{figure.caption.38}%
\contentsline {figure}{\numberline {A.2}{\ignorespaces Student model distillation convergence showing KL-divergence loss (blue), MSE loss (orange), and combined loss (green) over 100 training epochs.}}{64}{figure.caption.39}%
